{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# pyCCA\n",
    "\n",
    "#### 191218\n",
    "\n",
    "This notebook helps with doing Connected Concept Analysis in Python. It does (roughly) the same job as [Textometrica](http://textometrica.humlab.umu.se).\n",
    "\n",
    "For a description of CCA, refer to this paper:\n",
    "\n",
    ">Lindgren, S. (2016). [\"Introducing Connected Concept Analysis: A network approach to big text datasets\"](https://doi.org/10.1515/text-2016-0016). _Text & Talk: An Interdisciplinary Journal of Language, Discourse & Communication Studies_ 36(3): 341–362.\n",
    "\n",
    "CCA is a workflow for combining manual thematic coding with a form of [NTA](http://www.casos.cs.cmu.edu/publications/protected/1995-1999/1995-1997/carley_1997_networktext.PDF)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Setup\n",
    "\n",
    "Import the required libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare for stopword removal.\n",
    "\n",
    "Set as `keep_words` a list of standard stopwords, but that should not be removed.\n",
    "\n",
    "Set as `extra_stops` a list of non standard stopwords that should be removed.\n",
    "\n",
    "Stopwords will be loaded from nltk, removing the `keep_words` from the stoplist, and adding the `extra_stops` to it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up stopwords\n",
    "keep_words = [\"of\"] # name standard stopwords that should not be removed\n",
    "stops = [i for i in stopwords.words('english') if not i in keep_words] # load standard stopwords\n",
    "extra_stops = [\"via\", \"thats\", \"i’ve\", \"dont\", \"cant\",\"rt\", \"com\", \"twitter\", \"http\", \"https\",\\\n",
    "               \"pic\", \"www\",\"de\",\"fb\",\"en\",\"ly\",\"html\", \"status\", \"us\", \"es\", \"st\", \"tt\", \"la\"\\\n",
    "              \"li\",\"se\", \"bit\", \"ly\"] # name non-standard stopwords to be removed\n",
    "for i in extra_stops:\n",
    "    stops.append(i)\n",
    "stops = frozenset(stops)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import the corpus from a file with one document per line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [doc.strip() for doc in open(\"docs.txt\")]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Vectorize and count\n",
    "\n",
    "Vectorize the corpus, while removing stopwords, and only keeping words with >2 letters, and no numerical or special characters.\n",
    "\n",
    "We use `ngram_range = (1,3)` to get unigrams, bigrams, and trigrams."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Vectorize and count\n",
    "cv = CountVectorizer(ngram_range=(1, 3),\n",
    "                     strip_accents = 'unicode',\n",
    "                     stop_words=stops,\n",
    "                     token_pattern=\"[a-zA-Z][a-zA-Z]+\") # at least two letters, and no numerical or special characters\n",
    "dtm = cv.fit_transform(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get an ordered list of all token names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordlist = cv.get_feature_names()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get document frequencies for tokens (i.e. how many documents they occur in, no matter how many times)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docfreqs = list(np.squeeze(np.asarray((dtm != 0).sum(0)))) # count number of non-zero document occurrences for each row (i.e. each word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save token idnumbers, token names, and document frequencies in a dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "countsDF = pd.DataFrame(zip(wordlist,docfreqs)).reset_index()\n",
    "countsDF.columns = [\"id\",\"token\", \"DF\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keep only the top 3000 tokens, by document frequency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "countsDF = countsDF.sort_values(by=\"DF\", ascending=False).head(3000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Present the user with a list of top tokens (document frequency) in the corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"words.txt\", \"w\") as f:\n",
    "    for term in countsDF.token:\n",
    "        f.write(term + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Word selection\n",
    "\n",
    "<hr>\n",
    "**Before continuing:**\n",
    "\n",
    "- Open the `words.txt` file in an editor and delete all lines with words that you do _not_ want to keep for analysis. \n",
    "- Save the file with the same name, in the same directory.\n",
    "\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Define concepts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis_words = [w.strip() for w in open(\"words.txt\", \"r\").readlines()]\n",
    "df = pd.DataFrame(list(zip(analysis_words, analysis_words)), columns=['word','concept'])\n",
    "df.to_csv(\"concepts.csv\", index=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "**Before continuing:**\n",
    "\n",
    "- Open the `concepts.csv` file in an editor and enter concept names in the second column. \n",
    "- Leave the header row (`word,concept`) as it is.\n",
    "- If the word should not belong to a conceptual category, leave it as it is. \n",
    "- If you want to exclude the word from analysis, delete its entire row.\n",
    "- Save the file with the same name in the same directory.\n",
    "\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokensDF = pd.DataFrame([w.strip() for w in open(\"words.txt\", \"r\").readlines()])\n",
    "tokensDF.columns = ['token']\n",
    "finalDF = pd.merge(tokensDF,countsDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenids_we_want = list(finalDF.id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. Get co-occurrences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get a smaller document-term matrix with only the tokens we want."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtm2 = dtm[:, tokenids_we_want]\n",
    "dtm2.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get co-occurrences, and write as `networkx` graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cooc_matrix = (dtm2.T * dtm2) # this is cooccurrence matrix in sparse csr format\n",
    "cooc_matrix.setdiag(0) # fill same word cooccurence to 0\n",
    "cooc_matrix = cooc_matrix.todense() # convert sparse to dense"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6. Graph preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G=nx.from_numpy_matrix(cooc_matrix)\n",
    "print(nx.info(G))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "Keep only edges with a weight > than the `cutoff`.\n",
    "\n",
    "The cell below can be iterated with different cutoffs to see the size of the resulting `G2` graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cutoff = 1\n",
    "top = [edge for edge in G.edges_iter(data=True) \n",
    "       if edge[2]['weight'] > cutoff]\n",
    "G2 = nx.Graph(top)\n",
    "print(nx.info(G2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "Replace the numeric token labels with full text versions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = finalDF.token # an iterable of labels (in the right order)\n",
    "H = nx.relabel_nodes(G2, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the concepts.csv into a dictionary\n",
    "import csv\n",
    "\n",
    "reader = csv.reader(open(\"concepts.csv\"))\n",
    "next(reader, None) # skip the header line in the file\n",
    "newlabels = {}\n",
    "for row in reader:\n",
    "    key = row[0]\n",
    "    value = row[1]\n",
    "    newlabels.update( {key : value} )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "I = nx.relabel_nodes(H, newlabels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the graphs in Gephi format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nx.write_gexf(H, \"pyCCA_words.gexf\")\n",
    "nx.write_gexf(I, \"pyCCA_concepts.gexf\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
