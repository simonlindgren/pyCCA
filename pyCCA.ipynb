{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vzYtC_1GsVcF"
   },
   "source": [
    "# pyCCA\n",
    "\n",
    "#### 210508\n",
    "\n",
    "This notebook helps with doing Connected Concept Analysis in Python. It does (roughly) the same job as [Textometrica](http://textometrica.humlab.umu.se).\n",
    "\n",
    "For a description of CCA, refer to this paper:\n",
    "\n",
    ">Lindgren, S. (2016). [\"Introducing Connected Concept Analysis: A network approach to big text datasets\"](https://doi.org/10.1515/text-2016-0016). _Text & Talk: An Interdisciplinary Journal of Language, Discourse & Communication Studies_ 36(3): 341â€“362.\n",
    "\n",
    "CCA is a workflow for combining manual thematic coding with a form of [NTA](http://www.casos.cs.cmu.edu/publications/protected/1995-1999/1995-1997/carley_1997_networktext.PDF)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4jHz6sMIsVcH"
   },
   "source": [
    "#### 1. Setup\n",
    "\n",
    "Import the required libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true,
    "id": "8rUeqw8GuuSQ",
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "import networkx as nx\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Zfr1DDfYsVcI"
   },
   "source": [
    "Import English stopword list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "M_5zrJRouXgo",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "b2bf8cad-1417-4ccf-e0fc-c99fd425625a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "stops = [i for i in stopwords.words('english')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sI7xbWe1vN2T"
   },
   "source": [
    "Import Swedish stopword list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true,
    "id": "vHwwyalCvCUA",
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "response = requests.get('https://raw.githubusercontent.com/simonlindgren/pyCCA/master/swestops.txt')\n",
    "swestops = response.text.splitlines()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UlorpC0wwtJ7"
   },
   "source": [
    "Set your desired stopword list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true,
    "id": "ODMfpbMJwtd4",
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "#stops = swestops\n",
    "stops = stops"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fCOZo0MBsVcI"
   },
   "source": [
    "Tweak stopword removal.\n",
    "\n",
    "Set as `keep_words` a list of standard stopwords, but that should not be removed.\n",
    "\n",
    "Set as `extra_stops` a list of non standard stopwords that should be removed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true,
    "id": "pMhoLyKjsVcJ",
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Set up stopwords\n",
    "keep_words = [] # name standard stopwords that should not be removed\n",
    "stops = [i for i in stops if not i in keep_words] # load standard stopwords\n",
    "extra_stops = [] # name non-standard stopwords to be removed\n",
    "for i in extra_stops:\n",
    "    stops.append(i)\n",
    "stops = frozenset(stops)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6q4tViyosVcK"
   },
   "source": [
    "Import the corpus from a file with one document per line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "bXT-X920sVcK",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "1a5d3fd6-d79f-4787-c758-ebf4a3aa3cfa"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1645"
      ]
     },
     "execution_count": 29,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus = [doc.strip() for doc in open(\"docs.txt\")]\n",
    "len(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AWbtDkcisVcL"
   },
   "source": [
    "#### 2. Vectorize and count\n",
    "\n",
    "Vectorize the corpus, while removing stopwords, and only keeping words with >2 letters, and no numerical or special characters.\n",
    "\n",
    "We use `ngram_range = (1,2)` to get unigrams and bigrams."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "ildroDiMsVcL",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "266f221e-6244-4f54-e85d-77fd9b76a06a"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['adjo', 'alltsa', 'an', 'anda', 'annu', 'ar', 'aret', 'at', 'atminstone', 'atta', 'attio', 'attionde', 'attonde', 'aven', 'bada', 'badas', 'bade', 'bast', 'battre', 'behova', 'behovas', 'behovde', 'behovt', 'da', 'dar', 'darfor', 'fa', 'far', 'fatt', 'fjarde', 'foljande', 'for', 'fore', 'forlat', 'forra', 'forsta', 'framfor', 'fran', 'ga', 'galla', 'galler', 'gallt', 'gang', 'gar', 'garna', 'gatt', 'gor', 'gora', 'hjalp', 'hog', 'hoger', 'hogre', 'hogst', 'igar', 'infor', 'jamfort', 'kor', 'lagga', 'lange', 'langre', 'langsam', 'langsammare', 'langsammast', 'langsamt', 'langst', 'langt', 'latt', 'lattare', 'lattast', 'likstalld', 'likstallda', 'manga', 'maste', 'mojlig', 'mojligen', 'mojligt', 'mojligtvis', 'nagon', 'nagonting', 'nagot', 'nagra', 'nan', 'nar', 'nasta', 'nat', 'nodvandig', 'nodvandiga', 'nodvandigt', 'nodvandigtvis', 'ocksa', 'over', 'overmorgon', 'overst', 'ovre', 'pa', 'ratt', 'sa', 'sadan', 'sadana', 'sadant', 'sag', 'saga', 'sager', 'samre', 'samst', 'satt', 'sjalv', 'sjatte', 'sma', 'smatt', 'star', 'storre', 'storst', 'tjugotva', 'tva', 'tvahundra', 'ursakt', 'utanfor', 'val', 'vanster', 'vanstra', 'varfor', 'varifran', 'varre', 'varsagod'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.81 s, sys: 30.8 ms, total: 2.84 s\n",
      "Wall time: 2.85 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Vectorize and count\n",
    "cv = CountVectorizer(ngram_range=(1, 2),\n",
    "                     strip_accents = 'unicode',\n",
    "                     stop_words=stops,\n",
    "                     token_pattern=\"[a-zA-Z][a-zA-Z]+\") # at least two letters, and no numerical or special characters\n",
    "dtm = cv.fit_transform(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Cnyv9Ky1sVcL"
   },
   "source": [
    "Get an ordered list of all token names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true,
    "id": "0_74-AFwsVcL",
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "wordlist = cv.get_feature_names()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ih9HeHQTsVcM"
   },
   "source": [
    "Get document frequencies for tokens (i.e. how many documents they occur in, no matter how many times)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true,
    "id": "YIp0_cNysVcN",
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "docfreqs = list(np.squeeze(np.asarray((dtm != 0).sum(0)))) # count number of non-zero document occurrences for each row (i.e. each word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rmHtRQCqsVcN"
   },
   "source": [
    "Save token idnumbers, token names, and document frequencies in a dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true,
    "id": "asEwsJ-usVcN",
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "countsDF = pd.DataFrame(zip(wordlist,docfreqs)).reset_index()\n",
    "countsDF.columns = [\"id\",\"token\", \"DF\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wxl5-YLnsVcN"
   },
   "source": [
    "Keep only the top 3000 tokens, by document frequency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true,
    "id": "Iy3jWQKSsVcO",
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "countsDF = countsDF.sort_values(by=\"DF\", ascending=False).head(3000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kmxqHLC8sVcO"
   },
   "source": [
    "Present the user with a list of top tokens (document frequency) in the corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true,
    "id": "tG1M-OECsVcO",
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "with open(\"words.txt\", \"w\") as f:\n",
    "    for term in countsDF.token:\n",
    "        f.write(term + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ehCp1-vXsVcP"
   },
   "source": [
    "#### 3. Word selection\n",
    "\n",
    "<hr>\n",
    "**Before continuing:**\n",
    "\n",
    "- Open the `words.txt` file in an editor and delete all lines with words that you do _not_ want to keep for analysis. \n",
    "- Save the file with the same name, in the same directory.\n",
    "\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uCG5lncKsVcP"
   },
   "source": [
    "#### 4. Define concepts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true,
    "id": "gk3xO17ysVcP",
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "analysis_words = [w.strip() for w in open(\"words.txt\", \"r\").readlines()]\n",
    "df = pd.DataFrame(list(zip(analysis_words, analysis_words)), columns=['word','concept'])\n",
    "df.to_csv(\"concepts.csv\", index=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OKPW7Cy2sVcP"
   },
   "source": [
    "<hr>\n",
    "**Before continuing:**\n",
    "\n",
    "- Open the `concepts.csv` file in an editor and enter concept names in the second column. \n",
    "- Leave the header row (`word,concept`) as it is.\n",
    "- If the word should not belong to a conceptual category, leave it as it is. \n",
    "- If you want to exclude the word from analysis, delete its entire row.\n",
    "- Save the file with the same name in the same directory.\n",
    "\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true,
    "id": "wFBJA_3ysVcQ",
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "tokensDF = pd.DataFrame([w.strip() for w in open(\"words.txt\", \"r\").readlines()])\n",
    "tokensDF.columns = ['token']\n",
    "finalDF = pd.merge(tokensDF,countsDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true,
    "id": "krLPV8llsVcQ",
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "tokenids_we_want = list(finalDF.id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WP6P2usxsVcQ"
   },
   "source": [
    "#### 5. Get co-occurrences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dzinsjeqsVcR"
   },
   "source": [
    "Get a smaller document-term matrix with only the tokens we want."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "5oZ9dkX-sVcR",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "3053ed3f-8af8-4495-b5ff-520c811477cc"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1645, 43)"
      ]
     },
     "execution_count": 40,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dtm2 = dtm[:, tokenids_we_want]\n",
    "dtm2.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AXCh12XXsVcR"
   },
   "source": [
    "Get co-occurrences, and write as `networkx` graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true,
    "id": "-5js8ek9sVcR",
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "cooc_matrix = (dtm2.T * dtm2) # this is cooccurrence matrix in sparse csr format\n",
    "cooc_matrix.setdiag(0) # fill same word cooccurence to 0\n",
    "cooc_matrix = cooc_matrix.todense() # convert sparse to dense"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XTf7USz3sVcR"
   },
   "source": [
    "#### 6. Graph preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "9W4BPY5tsVcS",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "5f76c3ba-77b7-46a8-ae72-214ba5db67ca"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: \n",
      "Type: Graph\n",
      "Number of nodes: 43\n",
      "Number of edges: 903\n",
      "Average degree:  42.0000\n"
     ]
    }
   ],
   "source": [
    "G=nx.from_numpy_matrix(cooc_matrix)\n",
    "print(nx.info(G))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SpuApVQvsVcS"
   },
   "source": [
    "<hr>\n",
    "Keep only edges with a weight > than the `cutoff`.\n",
    "\n",
    "The cell below can be iterated with different cutoffs to see the size of the resulting `G2` graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "2p31ibOTsVcS",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "3127fb5f-9039-48f6-ef2c-2f1f301dfbb9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: \n",
      "Type: Graph\n",
      "Number of nodes: 43\n",
      "Number of edges: 884\n",
      "Average degree:  41.1163\n"
     ]
    }
   ],
   "source": [
    "cutoff = 1\n",
    "top = [edge for edge in G.edges(data=True) \n",
    "       if edge[2]['weight'] > cutoff]\n",
    "G2 = nx.Graph(top)\n",
    "print(nx.info(G2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0js0SLqIsVcS"
   },
   "source": [
    "<hr>\n",
    "Replace the numeric token labels with full text versions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true,
    "id": "wWL5thT5sVcT",
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "labels = finalDF.token # an iterable of labels (in the right order)\n",
    "H = nx.relabel_nodes(G2, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true,
    "id": "JeirlLsAsVcT",
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Read the concepts.csv into a dictionary\n",
    "import csv\n",
    "\n",
    "reader = csv.reader(open(\"concepts.csv\"))\n",
    "next(reader, None) # skip the header line in the file\n",
    "newlabels = {}\n",
    "for row in reader:\n",
    "    key = row[0]\n",
    "    value = row[1]\n",
    "    newlabels.update( {key : value} )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true,
    "id": "2iGLSEF9sVcU",
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "I = nx.relabel_nodes(H, newlabels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h73xu0uGsVcU"
   },
   "source": [
    "Save the graphs in Gephi format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true,
    "id": "1OciifoIsVcU",
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "nx.write_gexf(H, \"pyCCA_words.gexf\")\n",
    "nx.write_gexf(I, \"pyCCA_concepts.gexf\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "pyCCA.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
