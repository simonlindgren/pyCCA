{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# pyCCA\n",
    "\n",
    "This notebook helps with doing Connected Concept Analysis in Python. It does (roughly) the same job as [Textometrica](http://textometrica.humlab.umu.se).\n",
    "\n",
    "For a description of CCA, refer to this paper:\n",
    "\n",
    ">Lindgren, S. (2016). \"Introducing Connected Concept Analysis: A network approach to big text datasets\". _Text & Talk: An Interdisciplinary Journal of Language, Discourse & Communication Studies_ 36(3): 341–362. [doi](https://doi.org/10.1515/text-2016-0016\n",
    "\n",
    "CCA is a workflow for combining manual [thematic coding](http://dx.doi.org/10.1191/1478088706qp063oa) with a form of [NTA](http://www.casos.cs.cmu.edu/publications/protected/1995-1999/1995-1997/carley_1997_networktext.PDF).\n",
    "\n",
    "First, import the required libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import itertools\n",
    "import networkx as nx\n",
    "import plotly\n",
    "import glob\n",
    "\n",
    "from nltk.util import ngrams\n",
    "from nltk.corpus import stopwords\n",
    "stop = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Put a file named `docs.txt` in the working directory of this notebook. The file should have one document per line.\n",
    "\n",
    "Import, tokenize and clean the file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = [d.lower() for d in open(\"docs.txt\", \"r\").readlines()]\n",
    "docs = [[w.strip() for w in d.split()] for d in docs]\n",
    "\n",
    "# Perform a series of cleaning operations\n",
    "dks = []\n",
    "\n",
    "for doc in docs:\n",
    "    tks = [re.sub(\"\\.|,|\\:|/|\\\"|\\?|-|…|'|\\(|\\)|\\!|\\+\",\"\", t) for t in doc]\n",
    "    tks = [t for t in tks if not t.startswith(\"http\")]\n",
    "    tks = [t for t in tks if not \"http\" in t]\n",
    "    tks = [t for t in tks if not t.startswith(\"pictwitter\")]\n",
    "    tks = [t for t in tks if len(t) > 1 or t == \"i\"]\n",
    "    tks = [t for t in tks if not t.startswith(\"#\")]\n",
    "    tks = [t for t in tks if not t.startswith(\"@\")]\n",
    "    tks = [t for t in tks if not t.isnumeric()]\n",
    "    tks = [t for t in tks if len(t) > 2]\n",
    "    tks = [t for t in tks if len(t) < 25]\n",
    "    dks.append(tks)\n",
    "\n",
    "docs = dks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract and add ngrams (bigrams and trigrams)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs2 = []\n",
    "\n",
    "for doc in docs:\n",
    "    \n",
    "    tokenlist = []\n",
    "    \n",
    "    for word in doc:\n",
    "        tokenlist.append(word)\n",
    "    \n",
    "    bigrams=ngrams(doc,2)\n",
    "    trigrams=ngrams(doc,3)\n",
    "    \n",
    "    bigramlist = []\n",
    "    for b in bigrams:\n",
    "        bg = b[0] + \"_\" + b[1]\n",
    "        #print(bg)\n",
    "        tokenlist.append(bg)\n",
    "        \n",
    "    \n",
    "    trigramlist = []\n",
    "    for t in trigrams:\n",
    "        tg = t[0] + \"_\" + t[1] + \"_\" + t[2]\n",
    "        #print(tg)\n",
    "        tokenlist.append(tg)\n",
    "    \n",
    "    docs2.append(tokenlist)\n",
    "    \n",
    "docs = docs2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Present the user with a list of top tokens (document frequency) in the corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop = list(stop)\n",
    "\n",
    "manual_stopwords = [\"see\", \"get\"] # ADD MANUAL STOPWORDS HERE!\n",
    "for ms in manual_stopwords:\n",
    "    stop.append(ms)\n",
    "\n",
    "\n",
    "words = [w for doc in docs for w in set(doc) if w not in stop]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Added bit to process long word lists in RAM-friendly chunks\n",
    "\n",
    "# Define chunking function to yield successive n-sized chunks from a list\n",
    "def chunks(lst, n):\n",
    "    for i in range(0, len(lst), n):\n",
    "        yield lst[i:i + n]\n",
    "\n",
    "# Use the function to divide the list into chunks of 50000 words each        \n",
    "words_chunked = chunks(words, 50000)\n",
    "\n",
    "\n",
    "# Iterate over chunks and create a sub-dataframe with word frequencies for each chunk\n",
    "# Pickle each df do disk\n",
    "\n",
    "try:\n",
    "    os.mkdir(\"datapickles\")\n",
    "except: pass\n",
    "\n",
    "for c, chunk in enumerate(words_chunked):\n",
    "    wordcounts = np.unique(chunk, return_counts=True)\n",
    "    counted_words = wordcounts[0].tolist()\n",
    "    word_counts = wordcounts[1].tolist()\n",
    "    df = pd.DataFrame(list(zip(counted_words, word_counts)), columns=['word','DF']).sort_values(by=\"DF\", ascending=False).reset_index(drop=True)   \n",
    "    df.to_pickle(\"datapickles/dataframe\" + str(c) + \".pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get pickled frames\n",
    "pickled_frames = glob.glob(\"datapickles/*.pkl\")\n",
    "\n",
    "# Create an empty dataframe\n",
    "big_df = pd.DataFrame()\n",
    "\n",
    "# Join all dataframes into the big_df\n",
    "for pf in pickled_frames:\n",
    "    df = pd.read_pickle(pf)\n",
    "    keepers = df['DF'] > 19\n",
    "    df = df[keepers]   \n",
    "    big_df = pd.concat([big_df, df])\n",
    "\n",
    "df = big_df.groupby(['word'])['DF'].sum().reset_index().sort_values(by=\"DF\", ascending = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: delete all files in datapickles directory\n",
    "\n",
    "#for f in glob.glob(\"datapickles/*\"):\n",
    "#    os.remove(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>DF</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>119</th>\n",
       "      <td>women</td>\n",
       "      <td>235</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>men</td>\n",
       "      <td>196</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>sexual</td>\n",
       "      <td>186</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>know</td>\n",
       "      <td>123</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>many</td>\n",
       "      <td>107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>stories</td>\n",
       "      <td>103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>sexually</td>\n",
       "      <td>94</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>one</td>\n",
       "      <td>92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>assault</td>\n",
       "      <td>90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>people</td>\n",
       "      <td>90</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         word   DF\n",
       "119     women  235\n",
       "56        men  196\n",
       "79     sexual  186\n",
       "48       know  123\n",
       "55       many  107\n",
       "91    stories  103\n",
       "82   sexually   94\n",
       "64        one   92\n",
       "9     assault   90\n",
       "66     people   90"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(\"words.txt\", \"w\") as f:\n",
    "    for w in df.word:\n",
    "        f.write(w + \"\\n\")\n",
    "\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "**Before continuing:**\n",
    "\n",
    "- Open the `words.txt` file in an editor and delete all lines with words that you do _not_ want to keep for analysis. \n",
    "- Save the file with the same name, in the same directory.\n",
    "\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Now, continue** and prepare for conceptualisation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis_words = [w.strip() for w in open(\"words.txt\", \"r\").readlines()]\n",
    "df = pd.DataFrame(list(zip(analysis_words, analysis_words)), columns=['word','concept'])\n",
    "df.to_csv(\"concepts.csv\", index=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "**Before continuing:**\n",
    "\n",
    "- Open the `concepts.csv` file in an editor and enter concept names in the second column. \n",
    "- Leave the header row (`word,concept`) as it is.\n",
    "- If the word should not belong to a conceptual category, leave it as it is. \n",
    "- If you want to exclude the word from analysis, delete its entire row.\n",
    "- Save the file with the same name in the same directory.\n",
    "\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Now, continue** and prepare for network analysis.\n",
    "\n",
    "First, get all co-occurrence pairs by going through each `doc`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing document 4000/4000"
     ]
    }
   ],
   "source": [
    "df = pd.DataFrame.from_csv(\"concepts.csv\", index_col=None)\n",
    "\n",
    "co_occurrences = []\n",
    "\n",
    "for count,doc in enumerate(docs):\n",
    "    print(\"\\rProcessing document \" + str(count+1) + \"/\" + str(len(docs)), end=\"\")\n",
    "    matches = []\n",
    "    for word,concept in zip(df.word.tolist(),df.concept.tolist()):\n",
    "        if word in doc:\n",
    "            matches.append(concept)\n",
    "    if (len(set(matches))) > 1:\n",
    "        co_occurrences.append(list(set(matches)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How many of the documents included co-occurrences?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2247 out of 4000\n"
     ]
    }
   ],
   "source": [
    "print(str(len(co_occurrences)) + \" out of \" + str(len(docs)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some examples of co-occurrences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['join', 'like', 'twitter', 'happening', 'theres', 'with_you', 'thats']\n",
      "['wearing', 'started', 'going', 'every', 'day', 'getting', 'think', 'every_day']\n",
      "['read', 'you_can', 'enough', 'start', 'safe', 'feel', 'brave', 'hope', 'help']\n",
      "['someone', 'really', 'needs']\n",
      "['husband', 'better', 'dear', 'future']\n",
      "['next', 'and_then', 'for_the']\n",
      "['twitter', 'good', 'later']\n"
     ]
    }
   ],
   "source": [
    "for cooc in co_occurrences[:7]:\n",
    "    print(cooc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As seen above, our co-occurrences are lists of matches. But for network visualisation, we want pairs.\n",
    "\n",
    "Therefore, we go through all `co_occurrences` and use the `itertools` library to find all pairwise combinations within our co-occurrence-lists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing co-occurrence 2247/2247"
     ]
    }
   ],
   "source": [
    "sources = []\n",
    "targets = []\n",
    "\n",
    "for count,c in enumerate(co_occurrences):\n",
    "    print(\"\\rProcessing co-occurrence \" + str(count+1) + \"/\" + str(len(co_occurrences)), end=\"\")\n",
    "    pairs = itertools.combinations(c,2)\n",
    "    for p in list(pairs):\n",
    "        sources.append(p[0])\n",
    "        targets.append(p[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some example edges."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "join;like\n",
      "join;twitter\n",
      "join;happening\n",
      "join;theres\n",
      "join;with_you\n",
      "join;thats\n",
      "like;twitter\n"
     ]
    }
   ],
   "source": [
    "shortlist = list(zip(sources,targets))\n",
    "\n",
    "for source,target in shortlist[:7]:\n",
    "    print(source + \";\" + target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We write the full list of such pairs to a file named `cca.csv` that can be opened in Gephi as an edgelist."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "edges_df = pd.DataFrame(list(zip(sources, targets)), columns=['source','target'])\n",
    "edges_df.to_csv(\"cca.csv\", index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from networkx import set_node_attributes,betweenness_centrality\n",
    "from pyd3netviz import ForceChart\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First create a multigraph to allow multiple edges between the same pairs of nodes\n",
    "edgelist = pd.read_csv('cca.csv')\n",
    "M = nx.from_pandas_dataframe(edgelist, 'source', 'target', create_using = nx.MultiGraph())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create weighted graph from M\n",
    "G = nx.Graph()\n",
    "for u,v,data in M.edges(data=True):\n",
    "    w = data['weight'] if 'weight' in data else 1.0\n",
    "    if G.has_edge(u,v):\n",
    "        G[u][v]['weight'] += w\n",
    "    else:\n",
    "        G.add_edge(u, v, weight=w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<networkx.classes.graph.Graph at 0x114615f98>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "G"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"900\"\n",
       "            height=\"700\"\n",
       "            src=\"temp.html\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x11a425a90>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fc =ForceChart(G,charge=-100,link_distance=50, link_color_field='link_color',\n",
    "              node_color_field='node_color', node_radius_field='node_size')\n",
    "fc.to_notebook()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
